8/2021 First extraction TLDR:

  Two typo bugs had to be fixed in the spreadsheet itself
  Thinned to a smaller scenario set
  Numerical instability in 3rd poly TAM trend with only three datapoints; fixed by copying Excel data
  Standard helper tables fixes:
    copy_pds_to_ref=False,
    copy_ref_datapoint=False, copy_pds_datapoint=False,
  Unit adoption quirk fix:
    replacement_period_offset=0
  There are a bunch of test failures that I am leaving intact, because they are a result
  of inconsistent behavior when the TAM is negative.  Since TAM shouldn't *be* negative,
  I'm not going to worry about making the behavior consistent in this case.

Completing the extraction:

I have updated the tests to the new test format.
The original PR (listed below) had an extensive list of tests that were skipped,
but I am not reproducing that here.
With the new tests in place, the first error I see is 
AssertionError: Solution: Household & Commercial Recycling Scenario: PDS1-52p2050-Postint.Aug2019 TAM Data CI677:CM723
229/235 values differ:
 [0, 3]: expected 102.44152727 vs actual 98.13716006858265
 [0, 4]: expected 102.44152727 vs actual 98.13716006858265
 [1, 0]: expected -0.81258955 vs actual -0.09519501143043405
 [1, 1]: expected 3.23840227 vs actual -1.0659649314173731
 [1, 2]: expected 0.0 vs actual 7.891339874265168
 [1, 3]: expected 102.44152727 vs actual 98.13716006858265
 [2, 0]: expected -6.50071636 vs actual -0.7615600914434724
 [2, 1]: expected 12.95360909 vs actual -4.263859725669493
 [2, 2]: expected 0.0 vs actual 15.782679748530336
 [2, 3]: expected 102.44152727 vs actual 98.13716006858265
  ....

This is due to the numerical instability mentioned by the hackathon folks below.  I have
invented a new fix for this situation: I've copied the relevant table out of the Excel, stored
it as a CSV, and allowed for an override capability (currently for TAM; may do this later for
other cases if we encounter them).  There are limitations with this approach: (a) if one were
to update the TAM data sources, you have to remember to either update or remove the override,
and (b) I did it for 3rd poly only, which means that I had to disable the tests for the other
trends.

In addition to being unstable, the forecast TAM results dip into the negative starting in 2021.
This leads to some strange behavior in the Excel because it bounds the adoption by the TAM, so
it, too, is negative for the USA in those years.  Python computes the min(nan,-number) = nan,
and thus gets different results.  It does not seem worthwhile trying to match this edge case
as it is clearly undesirable behavior in the first place.

As a consequence, there are quite a few expected test failures in Unit Adoption (though oddly
they don't seem to propagate past that into First Cost, etc.)   I am not adding them to the
test_skip list, so this will just show up as having errors.  When we get key_results tests,
we should focus on those.

7/2021 Copied notes from the Hackathon PR
(https://github.com/ProjectDrawdown/solutions/pull/353)

CHANGE original excel sheet
Sheet: Variable Meta-analysis
Cell: K779
Old value: kg CO2/t
New value: Original Units

CHANGE original excel sheet
Sheet: Custom PDS Adoption
Cells: B56, B490,
Old value: zh
New value: copy-paste the formula from the adjacent cell

REMOVE the following scenarios. They're missing from the 'Custom PDS Adoption' sheet.
- PDS1-65p2050-Medium Prognostication (Book Ed.1)
- PDS1-65p2050-Medium Prognostication (Updated)

(Paraphrasing: the original conversation has images:
They encountered issues with inconsistency between Excel and Python with the LINEST function)

I think having 3 data points for this order 3 polynomial is asking a lot of precision from not a lot of data.

We're open to any suggestions for the next steps.

Denton's reply:
I have notes from one other time this happened, while importing AlternativeCements_v1.1c_IntegrationJune2020.xlsm.
I think I ended up pulling in an Excel file of data to sidestep the problem altogether: 
https://github.com/ProjectDrawdown/solutions/blob/develop/solution/altcement/data.xlsx

The notes from the time are that sklearn's linear regression least squares implementation produced closer 
results to Excel, but still not within a floating point margin of error sufficient to produce the same
result. My belief was that Excel's LINEST was also operating in an unstable region but just didn't say so.

C Poe:
It seems if we add one more row of data, our LINEST() and np.polyfit() functions compute the same coefficients.